# PassL Dispatch Pipeline Architecture

This document serves as the technical reference for the core dispatch system in the `passL` backend. The system is designed to computationally ingest incoming delivery orders, dynamically bundle them using real-world OSRM routing heuristics into optimal multi-stop Jobs, and safely dispatch those Jobs to 100+ active drivers using cascaded geofenced waves without suffering from concurrency race conditions.

---

## 1. Core Modules

The system employs strict **Separation of Concerns**, cleanly grouping complex math, state tracking, and network APIs so they do not tangle:

*   **`orders/`**: Responsible exclusively for queueing incoming Orders, triggering the mathematical batching engine, evaluating OSRM permutations, and creating bundled `Jobs`.
*   **`drivers/`**: The definitive source of driver geography, capacity, and status. It performs distance calculations to map drivers into actionable data sets. 
*   **`dispatch/`**: The Orchestrator. It takes the output of `orders/` and the calculations of `drivers/`, broadcasts the notifications, and securely locks the assignments.
*   **`routing/`**: The physical map layer. It proxies communication exclusively to the external OSRM (Open Source Routing Machine) network to calculate actual drivable distances and ETAs, rather than relying on flawed "as the crow flies" Cartesian math.

---

## 2. The Order Batching Engine (`orders/batching`)

The system uses an **O(NÂ²) Greedy Insertion Heuristic** instead of an intensive O(N!) Permutation Calculator. This allows the system to infinitely scale, routinely bundling 5+ orders into a single delivery vector in milliseconds.

The batch pipeline (`engine.batched_orders()`) operates in 3 distinct steps:

1.  **Clustering** (`clustering.py`): Separates the pool of all `RAW` orders into smaller distinct buckets (e.g. Orders from the exact same Restaurant, or very localized clusters), drastically reducing the math the combinatorial engine has to run.
2.  **Evaluating Insertions** (`feasibility.py`): The core engine selects a single seed order, and recursively runs every remaining unbatched order against it to see the theoretical impact of adding it to the route. It uses the `TimeMatrixProvider` (OSRM) to calculate the `detour_ratio` of every combination. 
3.  **Scoring** (`scoring.py`): If the mathematical `detour_ratio` complies with the `BatchingPolicy`, it calculates the total real-world "Time Saved" by physically combining the vectors, adding age-weight advantages to older orders, before locking them into a `Job`.

---

## 3. The 5-Wave Dispatch Cascade (`dispatch/dispatcher.py`)

Once a `Job` is generated by the engine, the Dispatcher takes over. Because drivers physically move throughout the city, the Dispatcher dynamically filters all active drivers against the pickup node of the `Job`. 

It utilizes an **OSRM Drive-Time Filter** (`drivers/selection.py`):
*   **Wave 1**: Broadcast to drivers with a real-world driving ETA of < 3 minutes (`180s`). Wait `wave_timeout_seconds` (e.g. 30s) for a response.
*   **Wave 2**: Automatically widen the threshold to a 7-minute ETA (`420s`) and push to new drivers.
*   **Wave 3, 4, 5**: Continuously widen the ETA net until the maximum threshold allows the Job to be claimed.
*(Fallback)*: Graciously defaults to Cartesian Pythagorean math (< 2km, < 4km, etc.) if the `PreloadingTimeMatrixProvider` is offline.

### Database Lock Resolution
Given hundreds of drivers, two drivers pressing "Accept" on a pushed Job at the exact same millisecond is statistically probable. The Dispatcher's `resolve_driver_acceptance()` enforces distributed safety. 

Whenever an API accepts an assignment, the dispatcher requests an exclusive hardware-level system block on that specific Job ID. Whichever request secures the lock gets physically assigned, the Job gets marked `ACCEPTED`, and the system seamlessly fires a **Silent Revoke Offer Push Notification** to every other driver who received the broadcast to delete the card from their apps effortlessly.

---

## 4. Architectural State Machines

Because algorithms rely on perfectly controlled environments, the architecture explicitly relies on rigid "State Machines" (`dispatch/state_machines/`) rather than ad-hoc property swaps. 

### Order Transitions (`order_state.py`)
Enforces the one-way lifecycle flow:
1.  **`RAW`**: Newly created order.
2.  **`BATCHING`**: Locked during OSRM calculations to prevent dual-processing.
3.  **`READY`**: Merged into a `Job`. Passed to the 5-Wave Cascade.
4.  **`ASSIGNED`**: Driver accepted.
*(Edge Case)*: Provides a `break_down_job_to_raw()` failsafe. If a driver claims a Job of 3 orders and their car breaks down globally, the Job shatters, instantly plummeting the 3 vectors back into `RAW` state with age-preference to rebuild. 

### Driver Transitions (`driver_state.py`)
When assigned to a multi-stop job, the machine physically deducts the size of the bundle sequentially from the vehicle's `max_capacity`. Assuming **Continuous Route Chaining** is active, drivers who accept a 2-order bundle, but have a 4-bag trunk capacity, will technically still be labeled mathematically `AVAILABLE` for the engine to seamlessly append pickups to their active route vector. 

---

## 5. The Heartbeat: Rolling Horizon Manager (`orders/queue.py`)

The pipeline runs entirely automatically by hooking into the `RollingHorizonManager`, acting as the active chron job or background worker.

1.  Waking up dynamically, it scans every order sitting in the `RAW` repository calculating their `wait_time_seconds`. 
2.  If an order eclipses the configured `batching_soft_wait_sec` constraint, they are force-promoted to `BATCHING`. 
3.  The Manager injects them into the Combinatorial Engine, passing along the `BatchingPolicy` configurations. 
4.  Resulting `Jobs` instances are forcibly promoted into the `READY` queue so the Dispatcher can asynchronously sweep them up.

---

## 6. Central Configuration (The Policy Layer)

Instead of hardcoded variables spanning a dozen files, the algorithmic thresholds are governed exclusively by decoupled Dataclasses that can dynamically swap without restarting code:

*   **`orders/batching/policy.py:BatchingPolicy`**:
    *   `max_batch_size=10`
    *   `pair_detour_cap=1.15` and `multi_detour_cap=1.25`
    *   `batching_soft_wait_sec=180`
    *   `age_weight=0.05`
*   **`drivers/policy.py:DriverPolicy`**:
    *   `wave_timeout_seconds=30`
    *   `wave_radii_degrees=[0.02, 0.04, 0.06, 0.08, 0.10]`
